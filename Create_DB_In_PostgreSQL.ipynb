{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II -Fetch Distinct Values from MongoDB\n",
    "#### 1) Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all the required libraries\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "print(\"Imported all the required libraries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Connect/Initialize the MongoDB hosted on  \"cyclades.okeanos-global.grnet.gr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB instance initialized!\n"
     ]
    }
   ],
   "source": [
    "db_cl = MongoClient('83.212.82.56', 27017)\n",
    "db = db_cl.DAP_ProjectDB\n",
    "print(\"MongoDB instance initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) HRRP Collection : Identifing distinct measure names to pivot data from JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of measure names in HRRP : \n",
      "[]\n",
      "\n",
      "List of measure names in HRRP  after replacing '-' with '_' : \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# collection hrrp instance\n",
    "collection_hrrp = db.hrrp\n",
    "#collection hacrp instance\n",
    "collection_hacrp = db.hacrp\n",
    "#collection hvbp instance\n",
    "collection_hvbp = db.hvbp\n",
    "\n",
    "# fetching distinct types of measure name\n",
    "hrrp_measure_list_orig = db['hrrp'].distinct('measure_name')\n",
    "print(f\"List of measure names in HRRP : \\n{hrrp_measure_list_orig}\")\n",
    "# replacing \"-\" with \"_\" in measure names\n",
    "hrrp_measure_list = [w.replace('-', '_') for w in hrrp_measure_list_orig]\n",
    "print(f\"\\nList of measure names in HRRP  after replacing '-' with '_' : \\n{hrrp_measure_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Creating the PostgreSQL database (dap_medicare) hosted on the \"cyclades.okeanos-global.grnet.gr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All databases present previously :\n",
      "['postgres', 'template1', 'template0', 'dap_medicare']\n",
      "dap_medicare schema dropped!\n",
      "\n",
      "All databases present after check :\n",
      "['postgres', 'template1', 'template0']\n",
      "\n",
      "Created a new db schema 'dap_medicare'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dbConnection = psycopg2.connect(\n",
    "    user = \"dap\",\n",
    "    password = \"dap\",\n",
    "    host = \"83.212.82.56\",\n",
    "    port = \"5432\",\n",
    "    database = \"postgres\")\n",
    "    dbConnection.set_isolation_level(0) # AUTOCOMMIT\n",
    "    dbCursor = dbConnection.cursor()\n",
    "\n",
    "    # fetching the list of databases/schema present in the postgres instance\n",
    "    dbCursor.execute(\"SELECT datname from pg_database\")\n",
    "    rows = dbCursor.fetchall()\n",
    "    dbNames = []\n",
    "    for row in rows:\n",
    "        dbNames.append(row[0])\n",
    "    print(f\"\\nAll databases present previously :\\n{dbNames}\")\n",
    "\n",
    "    # checking if the database already exists/ if so then dropping the same\n",
    "    if(\"dap_medicare\" in dbNames) :\n",
    "        try:\n",
    "            dbCursor.execute(\"DROP DATABASE dap_medicare\")\n",
    "            print(\"dap_medicare schema dropped!\")\n",
    "        except:\n",
    "            print(\"Error while dropping the database!\")\n",
    "\n",
    "    # checking the database schema names after dropping\n",
    "    try:\n",
    "        dbCursor.execute(\"SELECT datname from pg_database\")\n",
    "    except:\n",
    "        print(\"Error while fetching database names\")\n",
    "    rows = dbCursor.fetchall()\n",
    "    dbNames = []\n",
    "    for row in rows:\n",
    "        dbNames.append(row[0])\n",
    "    print(f\"\\nAll databases present after check :\\n{dbNames}\")\n",
    "\n",
    "    # Creating new schema dap_medicare\n",
    "    try:\n",
    "        dbCursor.execute('CREATE DATABASE dap_medicare;')\n",
    "        print(\"\\nCreated a new db schema 'dap_medicare'\")\n",
    "    except:\n",
    "        print(\"Error while creating dap_medicare database!\")\n",
    "    dbCursor.close()\n",
    "except (psycopg2.Error) as dbError :\n",
    "    print(\"Error while connecting to PostgreSQL\", dbError)\n",
    "except Exception as exc :\n",
    "    print(\"Error while creating the database schema in PostgreSQL\", exc)\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Creating strings to create dynamic tables with base columns for different collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "createStringHRRP = \"\"\"\n",
    "DROP TABLE IF EXISTS \"{tbl_Name}\" CASCADE ;\n",
    "CREATE TABLE \"{tbl_Name}\"(\n",
    "date_time timestamp,\n",
    "hospital_name VARCHAR(100),\n",
    "provider_id integer PRIMARY KEY,\n",
    "state VARCHAR(2),\n",
    "measure_name VARCHAR,\n",
    "number_of_discharges integer,\n",
    "excess_readmission_ratio float,\n",
    "predicted_readmission_rate float,\n",
    "expected_readmission_rate float,\n",
    "number_of_readmissions integer,\n",
    "start_date timestamp,\n",
    "end_date timestamp\n",
    ");\n",
    "\"\"\"\n",
    "createStringHACRP = \"\"\"\n",
    "DROP TABLE IF EXISTS hacrp CASCADE ;\n",
    "CREATE TABLE hacrp(\n",
    "hospital_name VARCHAR(100),\n",
    "provider_id integer PRIMARY KEY,\n",
    "state VARCHAR(2),\n",
    "fiscal_year integer,\n",
    "psi_90__start_date timestamp,\n",
    "psi_90_end_date timestamp,\n",
    "psi_90_w_z_score float,\n",
    "clabsi_w_z_score float,\n",
    "cauti_w_z_score float,\n",
    "ssi_w_z_score float,\n",
    "mrsa_w_z_score float,\n",
    "cdi_w_z_score float,\n",
    "hai_measures_start_date timestamp,\n",
    "hai_measures_end_date timestamp,\n",
    "total_hac_score float,\n",
    "payment_reduction VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "createStringHVBP = \"\"\"\n",
    "DROP TABLE IF EXISTS hvbp CASCADE ;\n",
    "CREATE TABLE hvbp(\n",
    "hospital_name VARCHAR(100),\n",
    "provider_number integer PRIMARY KEY,\n",
    "address VARCHAR,\n",
    "city VARCHAR,\n",
    "state VARCHAR(2),\n",
    "zip_code integer,\n",
    "county_name VARCHAR,\n",
    "mort_30_ami_achievement_threshold float,\n",
    "mort_30_ami_benchmark float,\n",
    "mort_30_ami_baseline_rate float,\n",
    "mort_30_ami_performance_rate float,\n",
    "mort_30_ami_achievement_points integer,\n",
    "mort_30_ami_improvement_points integer,\n",
    "mort_30_ami_measure_score integer,\n",
    "mort_30_hf_achievement_threshold float,\n",
    "mort_30_hf_benchmark float,\n",
    "mort_30_hf_baseline_rate float,\n",
    "mort_30_hf_performance_rate float,\n",
    "mort_30_hf_achievement_points integer,\n",
    "mort_30_hf_improvement_points integer,\n",
    "mort_30_hf_measure_score integer,\n",
    "mort_30_pn_achievement_threshold float,\n",
    "mort_30_pn_benchmark float,\n",
    "mort_30_pn_baseline_rate float,\n",
    "mort_30_pn_performance_rate float,\n",
    "mort_30_pn_achievement_points integer,\n",
    "mort_30_pn_improvement_points integer,\n",
    "mort_30_pn_measure_score integer,\n",
    "comp_hip_knee_achievement_threshold float,\n",
    "comp_hip_knee_benchmark float,\n",
    "comp_hip_knee_baseline_rate float,\n",
    "comp_hip_knee_performance_rate float,\n",
    "comp_hip_knee_achievement_points integer,\n",
    "comp_hip_knee_improvement_points integer,\n",
    "comp_hip_knee_measure_score integer,\n",
    "lat float,\n",
    "long float);\n",
    "\"\"\"\n",
    "#print(f\"Dynamic create table String HRRP : \\n{createStringHRRP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Creating tables into postgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table READM_30_AMI_HRRP created\n",
      "Table READM_30_CABG_HRRP created\n",
      "Table READM_30_COPD_HRRP created\n",
      "Table READM_30_HF_HRRP created\n",
      "Table READM_30_HIP_KNEE_HRRP created\n",
      "Table READM_30_PN_HRRP created\n",
      "Table hacrp created\n",
      "Table hvbp created\n",
      "\n",
      "Successfully created 6 tables for HRRP collection\n",
      "\n",
      "Successfully created 1 table for HACRP collection\n",
      "\n",
      "Successfully created 1 table for HVBP collection\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dbConnection = psycopg2.connect(\n",
    "    user = \"dap\",\n",
    "    password = \"dap\",\n",
    "    host = \"83.212.82.56\",\n",
    "    port = \"5432\",\n",
    "    database = \"dap_medicare\")\n",
    "    dbConnection.set_isolation_level(0) # AUTOCOMMIT\n",
    "    dbCursor = dbConnection.cursor()\n",
    "\n",
    "    # creating tables for all collections\n",
    "    for i in range(len(hrrp_measure_list)):\n",
    "        dbCursor.execute(createStringHRRP.format(tbl_Name = hrrp_measure_list[i]))\n",
    "        print(f\"Table {hrrp_measure_list[i]} created\")\n",
    "    dbCursor.execute(createStringHACRP)\n",
    "    print(f\"Table hacrp created\")\n",
    "    dbCursor.execute(createStringHVBP)\n",
    "    print(f\"Table hvbp created\")\n",
    "    dbCursor.close()\n",
    "    print(f\"\\nSuccessfully created {len(hrrp_measure_list)} tables for HRRP collection\")\n",
    "    print(f\"\\nSuccessfully created 1 table for HACRP collection\")\n",
    "    print(f\"\\nSuccessfully created 1 table for HVBP collection\")\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error while table creation in PostgreSQL : \\n\", dbError)\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_imputer(df_import):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_import2 = pd.DataFrame(imputer.fit_transform(df_import))\n",
    "    df_import2.columns = df_import.columns\n",
    "    return df_import2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Scores and Points in HVBP Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_extract(df_import):\n",
    "    for column, data in df_import.filter(regex='points$|score$').iteritems():\n",
    "        df_import[column].replace(to_replace = '\\W.*', value= ' ', regex=True, inplace=True)\n",
    "    return df_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_coord(df_new):\n",
    "    lat = []\n",
    "    lon = []\n",
    "    for x in df_new['geocoded_column']:\n",
    "        if isinstance(x,dict):\n",
    "            lat.append(x['coordinates'][0])\n",
    "            lon.append(x['coordinates'][1])\n",
    "        else:\n",
    "            lat.append(np.nan)\n",
    "            lon.append(np.nan)\n",
    "    df_new['latitude']=lat\n",
    "    df_new['longitude']=lon\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9) Inserting data to tables into postgreSQL database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-544-ba546344cad0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"N/A\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Too Few to Report\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdf_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_imputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_cleaned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdf_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_imputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total_hac_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-204-f9e317963898>\u001b[0m in \u001b[0;36mdf_imputer\u001b[1;34m(df_import)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdf_imputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_import\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf_import2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_import\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdf_import2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_import\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_import2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_knn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         X = check_array(X, accept_sparse=False, dtype=FLOAT_DTYPES,\n\u001b[1;32m--> 182\u001b[1;33m                         force_all_finite=force_all_finite, copy=self.copy)\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_indicator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mdtype_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "engine = create_engine('postgresql://dap:dap@83.212.82.56:5432/dap_medicare', echo=True)\n",
    "\n",
    "# dynamically inserting data in measure tables for hrrp\n",
    "for i in range(len(hrrp_measure_list)):\n",
    "    cursor = collection_hrrp.find({\"measure_name\": hrrp_measure_list_orig[i]}, { \"_id\" : 0, \"footnote\" : 0}) #Fetch Data from MongoDB and filter with field 'measure_name'\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "    df.replace(\"N/A\", np.nan, inplace = True)\n",
    "    df.replace(\"Too Few to Report\", np.nan, inplace = True)\n",
    "    df_cleaned = df_imputer(df.iloc[:, 4:-2])\n",
    "    df.iloc[:, 4:-2]=df_cleaned\n",
    "    df.to_sql(hrrp_measure_list[i], engine, if_exists = 'append', chunksize = 100, index= False)\n",
    "    print(f\"Insertion completed in {hrrp_measure_list[i]} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Inserting Data to HACRP table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting data in hacrp table\n",
    "cursor = collection_hacrp.find({},{\"_id\" : 0, \"footnote\" : 0, \"payment_reduction\":0, \"clabsi_footnote\":0, \"ssi_footnote\":0, \"mrsa_footnote\":0, \"cauti_footnote\":0, \"cdi_footnote psi_90_footnote\":0, \"total_hac_footnote\":0, \"payment_reduction_footnote\":0,\"cdi_footnote\":0,\"psi_90_footnote\":0\n",
    "})\n",
    "df = pd.DataFrame(list(cursor)) \n",
    "df.replace(\"N/A\", np.nan, inplace = True)\n",
    "df.replace(\"Too Few to Report\", np.nan, inplace = True)\n",
    "df_cleaned = df_imputer(df.iloc[:, 6:12])\n",
    "df.iloc[:, 6:12]=df_cleaned\n",
    "df_cleaned = df_imputer(df[['total_hac_score']])\n",
    "df[['total_hac_score']]=df_cleaned\n",
    "df.to_sql(hacrp, engine, if_exists = 'append', chunksize = 100, index= False)\n",
    "print(f\"Insertion completed in hacrp table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) Inserting data into HVBP table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting data in hvbp table\n",
    "cursor = collection_hvbp.find({},{\"_id\" : 0, \"footnote\" : 0})\n",
    "df = pd.DataFrame(list(cursor))\n",
    "df.replace(\"Not Available\", -99999, inplace = True)\n",
    "df = df_extract(df)\n",
    "df.replace(-99999, np.nan, inplace = True)\n",
    "df.replace('0.896948(23)', 0.896948, inplace=True)\n",
    "df.iloc[:,7:-4]=df.iloc[:,7:-4].astype(float)\n",
    "df_cleaned = df_imputer(df.iloc[:,7:-4])\n",
    "df.iloc[:,7:-4]=df_cleaned\n",
    "df = df_coord(df)\n",
    "df = df.drop(df.iloc[:,35:39], axis=1)\n",
    "df.to_sql(hvbp, engine, if_exists = 'append', chunksize = 100, index= False)\n",
    "print(f\"Insertion completed in hvbp table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
